---
title: "continue....."
output:
  html_document: default
  pdf_document: default
bibliography: Andrea.bib
csl: apa.csl
---

```{r setup, results=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## 7. Data analysis 

First of all, we will make an introduction to **Chronic obstructive pulmonary disease (COPD)**.\

Chronic obstructive pulmonary disease (COPD) is a long-term lung illness in which the lungs' small airways are damaged, making it difficult for air to enter and exit.\

COPD patients commonly experience one or more of the following symptoms:

- having trouble breathing 

- prolonged cough (more than 3 months) 

- a cough with mucus most days

- wheezing (a whistling sound when you breathe)

- chest tightness


Shortness of breath is the most common symptom of COPD. Simple daily tasks like walking short distances or walking a stairway might cause COPD sufferers to become out of breath. Even at rest, breathing might become difficult as the condition progresses.\

Long-term cigarette smoking is the most common cause of COPD. The more cigarettes you smoke, the more likely you are to get COPD.\


<br>


### 7.1 Clinical data

Before we begin with our analysis, we need to explore our clinical data and clean it.

View data:
```{r, warning=FALSE}
clinic_data <- read.delim("eclipse_all.txt")
library(rmarkdown)
paged_table(clinic_data)
```

Add 0 in front of index to match rdr names:
```{r}
clinic_data$id <- paste0("0", clinic_data$id)
```

```{r}
clinic_data <- clinic_data[-1] # delete first columns (index 0,1,2...)
row.names(clinic_data) <- clinic_data$id # id as rownames
clinic_data$id <- NULL # delete id colum
paged_table(clinic_data)
```

Select variables of t1.
```{r, warning=FALSE, results = FALSE}
library(tidyverse)
```
```{r, warning=FALSE}
clinical_data_T1 <- dplyr::select(clinic_data, !ends_with("t3"))
```

Check for missing data:
```{r}
sum(is.na(clinical_data_T1))
```

Delete variables that have more than a 50% of missing values:
```{r}
more_50_missing <- names(clinical_data_T1[ , colMeans(is.na(clinical_data_T1)) > 0.5])
more_50_missing
clinical_data_T1 <- clinical_data_T1[, !colnames(clinical_data_T1) %in% more_50_missing]
```

Check for missing data:
```{r}
sum(is.na(clinical_data_T1))
```

Impute missing values of double variables for the mean value:
```{r}
clinical_data_T1 <- clinical_data_T1 %>%  mutate_if(is.double,
                                                    function(x) ifelse(is.na(x), mean(x, na.rm = T), x)) 
```

Impute missing values of integer variables for the most common value:
```{r}
clinical_data_T1 <- clinical_data_T1 %>%  mutate_if(is.integer,
                                                    function(x) ifelse(is.na(x), which.max(x), x)) 
```

Impute missing values of categroical variables for the most common value:
```{r}
clinical_data_T1[, sapply(clinical_data_T1, function(x) !is.numeric(x))] <- apply(clinical_data_T1[, sapply(clinical_data_T1, function(x) !is.numeric(x))], 
                                                      2, 
                                                      function(x) {x[is.na(x)] <- names(sort(table(x), decreasing = TRUE)[1]); x})
```

Check for missing data:
```{r}
sum(is.na(clinical_data_T1))
```


Characters to factors:
```{r}
ch <- sapply(clinical_data_T1, is.character)
clinical_data_T1[ch] <- lapply(clinical_data_T1[ch], as.factor)
```

Now that we have a clean data set, we will exported:
```{r}
write.table(clinical_data_T1, file = "eclipse_all_CLEAN.txt", sep = "\t")
```

```{r}
clinic_data_clean <- read.delim("eclipse_all_CLEAN.txt")
paged_table(clinic_data_clean) # view firsts values
```


<br>



### 7.2 Principal Component Analysis (PCA)

We will first start doing principal component analysis with the radiomic features.\

Recall that PCA is a statistical approach that allows you to summarize the content of big data tables using a smaller set of "summary indices" that are easier to display and study.\


For PCA we want only want categorical data.\


Load the clean data set:
```{r}
clinic_data_clean <- read.delim("eclipse_all_CLEAN.txt")
paged_table(clinic_data_clean) # view firsts values
```

Load the libraries needed to perform PCA:
```{r, message=FALSE, results = FALSE}
library(ade4)
library(made4)
library(scatterplot3d)
```

To do PCA, we need all columns to be numeric.
```{r}
data <- mutate_all(clinic_data_clean, function(x) as.numeric(as.factor(x)))
paged_table(data)
```

Perform PCA:
```{r}
results_PCA <- ord(data, type="coa")
```

```{r}
names(results_PCA)
```


```{r}
plot(results_PCA, classvec=colnames(data), genecol="grey3")
```


<br>



### 7.2 Canonical correlation analysis (CCA)

Now we will analyze the radiomic features against the variables at time T1 using **canonical correlation**.\

Canonical correlation analysis (CCA) is a statistical technique for extracting information from cross-covariance matrices. If we have two vectors of random variables, X = (X1,..., Xn) and Y = (Y1,..., Ym), and the variables are correlated, canonical-correlation analysis will find linear combinations of X and Y that have the highest correlation. \

Multiple correlation analysis predicts only one dependent variable from several independents, but canonical correlation predicts multiple dependent variables from multiple independents.\

In our case, we will have 2 tables. One with the radiomics features, and another one with the clinical data. We want to have id -> columns and features -> rows.\


Load the radar object and convert it to a data frame:
```{r, warning=FALSE, results = FALSE}
library(rmarkdown)
library(admisc)
file <- listRDA("rdr_L1_Andrea.rda")
rdr_L1 <- file$rdr_L1
df_rdr <- as.data.frame(assay(rdr_L1))
paged_table(df_rdr)
```

Load the clean data set:
```{r}
clean_data <- read.delim("eclipse_all_CLEAN.txt")
paged_table(clean_data) # view firsts values
```

To do CCA, we need all columns to be numeric.
```{r}
clean_data <- mutate_all(clean_data, function(x) as.numeric(as.factor(x)))
```

Now we want the id as columns instead of rows.
```{r}
df_clinical <- as.data.frame(t(clean_data))
colnames(df_clinical) <- rownames(clean_data) # put id names
paged_table(df_clinical)
```

Now we have both data frames. But they differ int the number of id. Select in data frames only if id match the common id.
```{r}
id_in_rdr <- colnames(df_rdr) # id of rdr
id_in_clinical <- colnames(df_clinical) # id of clinical
common_id <- Reduce(intersect, list(id_in_rdr,id_in_clinical)) # common id

df_clinical <- select(df_clinical, one_of(common_id))
df_rdr <- select(df_rdr, one_of(common_id))
```

Combine both data frames into a list.
```{r}
both_tables <- list("rdr"=df_rdr, "clinical"=df_clinical)
```

Load the library to do CCA:
```{r, warning=FALSE}
library(omicade4)
```

Check the dimension are correct:
```{r}
sapply(both_tables, dim)
```

Perform CCA:
```{r}
mcoin <- mcia(both_tables, cia.nf=10)
```

Visualize the result:
```{r}
plot(mcoin, axes=1:2, sample.lab=FALSE, df.color=1:2)
```


<br>



### 7.4 TRTGRP prediction

Now we will use different methods to predict *TRTGRP* with the radiomic features.\

Load the radar object and convert it to a data frame:
```{r, warning=FALSE, results = FALSE}
library(rmarkdown)
library(admisc)
```
```{r, warning=FALSE}
file <- listRDA("rdr_L1_Andrea.rda")
rdr_L1 <- file$rdr_L1
df_rdr <- as.data.frame(assay(rdr_L1))
paged_table(df_rdr)
```

Load the clean data set and convert characters to factors:
```{r}
clean_data <- read.delim("eclipse_all_CLEAN.txt")
ch <- sapply(clean_data, is.character)
clean_data[ch] <- lapply(clean_data[ch], as.factor)
paged_table(clean_data)
```

View a summary of our variable of interest *TRTGRP.t1*:
```{r}
table(clean_data$TRTGRP.t1)
```


We have three different levels: COPD Subjects Non-smoker, Controls and Smoker Controls.\

We are interested in predicting *TRTGRP.t1* given the radiomic features. So we will add this variable to the table of radiomic features.\

Since both tables differ in the number of id, we select in data frames only if id match the common id.

```{r, warning=FALSE}
library(tidyverse)
id_in_rdr <- colnames(df_rdr) # id of rdr
id_in_clinical <- rownames(clean_data) # id of clinical
common_id <- Reduce(intersect, list(id_in_rdr,id_in_clinical)) # common id
df_clinical <- clean_data %>% filter(row.names(clean_data) %in% common_id)
df_rdr <- select(df_rdr, one_of(common_id))
```

Convert *TRTGRP.t1* to numeric:
```{r}
df_clinical$TRTGRP.t1 <- as.numeric(df_clinical$TRTGRP.t1)
TRTGRP_values <- df_clinical$TRTGRP.t1
table(TRTGRP_values)
```

Add the values to the data frame:
```{r}
df_rdr2 <- rbind(df_rdr, "TRTGRP_values" = TRTGRP_values)
```

Now we want the id as rows instead of columns:
```{r}
df_rdr3 <- as.data.frame(t(df_rdr2))
rownames(df_rdr3) <- colnames(df_rdr2) # put id names
paged_table(df_rdr3)
```

```{r}
df_rdr3$TRTGRP_values <- as.factor(df_rdr3$TRTGRP_values)
levels(df_rdr3$TRTGRP_values) <- c("COPD Subjects", "Non-smoker Controls", "Smoker Controls")
```


With the data we have, we need to balance the data.

Check how much data we have of each type:
```{r, out.width="50%"}
barplot(prop.table(table(df_rdr3$TRTGRP_values)),
        col = rainbow(3), ylim = c(0, 1),
        main = "Class Distribution")
```

We can clearly see that we don't have balanced data, this will produce not very accurate results in our models.
```{r}
table(TRTGRP_values)
```


<br>



**Boosting**

We create a function that takes as a parameter your entire dataset and returns the accuracy of a boosting model (GBM).\

Load the libraries.
```{r, warning=FALSE, results = FALSE}
library(gbm)
library(caret)    
```

Function:
```{r}
Boosting <- function(data){
  
  #### Select train 70% and test 30% randomly
  rows <- sample(nrow(data), .7 * nrow(data))
  train_data <- data[rows, ] # train data
  test_data <- data[-rows, ] # test data
  
  
  #### Train a model using our training data
  model_gbm <- gbm(TRTGRP_values ~.,
                data = train_data,
                distribution = "multinomial",
                cv.folds = 10,
                shrinkage = .01,
                n.minobsinnode = 10,
                n.trees = 500)       # 500 tress to be built
  

  #### Make predictions on the test data
  pred_test <- predict.gbm(object = model_gbm,
                   newdata = test_data,
                   n.trees = 500,           # 500 tress to be built
                   type = "response")
  
  
  #### Create a confusion matrix:
  class_names <- colnames(pred_test)[apply(pred_test, 1, which.max)]
  result <- data.frame(test_data$TRTGRP_values, class_names)
  conf_mat <- confusionMatrix(test_data$TRTGRP_values, as.factor(class_names))
  
  return(conf_mat$overall["Accuracy"])
  
}
```

First we will do XgBoost *without balancing* the data:
```{r, warning=FALSE}
Boosting(df_rdr3)
```

We get an Accuracy of around 79%.

Now we balance our data with *SMOTE* method, which is an oversampling technique where the  samples are generated for the minority class. 

```{r}
smote_data <- SMOTE(TRTGRP_values ~ ., data  = df_rdr3)                         
table(smote_data$TRTGRP_values) 
```

```{r}
Boosting(smote_data)
```

We get worse results with an Accuracy of 70%.


<span style="color:red">Worse results balancing data?</span>



<br>



**Random Forest**

With Random Forest we can't have columns starting with numbers, so we will change them and add an A before the number:
```{r, warning=FALSE, results = FALSE}
library(data.table)
```

```{r}
df_rdr4 <- df_rdr3
# Columns that start with numbers
names_number <- colnames(df_rdr4 %>% dplyr:: select(starts_with(c("0","1","2","3","4","5","6","7","8","9"))))
# Add and A before the number
names_changed <- unlist(lapply(names_number, function(x) paste("A", x, sep="")))
# Change the names
setnames(df_rdr4, old=names_number, new = names_changed, skip_absent=TRUE)
paged_table(df_rdr4)
```


We create a function that takes as a parameter your entire dataset and returns the accuracy of a Random Forest model.\

Load the library.
```{r, warning=FALSE, results = FALSE}
library(randomForest)
```

Function:
```{r}
RandomForest <- function(data){
  
  #### Select train 70% and test 30% randomly
  rows <- sample(nrow(data), .7 * nrow(data))
  train_data <- data[rows, ]
  test_data <- data[-rows, ]
  
  
  #### Train a model using our training data
  model <- randomForest(TRTGRP_values ~ . , data = train_data)
  
  
  #### Make predictions on the test data
  predictions <- predict(model, test_data)
  confusion_matrix <- with(test_data, table(predictions, TRTGRP_values))
  
  
  #### Accuracy of our model
  accuracy <- 100 * sum(diag(confusion_matrix)) / sum(confusion_matrix)
  return(accuracy)
}
```

First we will do Random Forest *without balancing* the data:
```{r}
RandomForest(df_rdr4)
```

We get an Accuracy of around 79%.

```{r}
table(df_rdr4$TRTGRP_values)
```


Now we balance our data with *SMOTE* method, which is an oversampling technique where the  samples are generated for the minority class. 
```{r, warning=FALSE, results = FALSE}
# remotes::install_github("cran/DMwR")
library(DMwR)
```

```{r}
smote_data <- SMOTE(TRTGRP_values ~ ., data  = df_rdr4)                         
table(smote_data$TRTGRP_values) 
```

```{r}
RandomForest(smote_data)
```

We get better results with an Accuracy of around 84%.\



<br>




**XGBoost**

We create a function that takes as a parameter your entire dataset and returns the accuracy of a XgBoost model.\

Load the libraries.
```{r, warning=FALSE, results = FALSE}
library(xgboost)
library(caret)
```

Function:
```{r}
XgBoost <- function(data){
  
  #### Select train 70% and test 30% randomly
  rows <- sample(nrow(data), .7 * nrow(data))
  train_data <- data[rows, ] # train data
  test_data <- data[-rows, ] # test data
  
  
  ####  Independent and dependent variables for train and test
  X_train = data.matrix(train_data[,-ncol(data)]) 
  y_train = train_data[,ncol(data)] 
    
  X_test = data.matrix(test_data[,-ncol(data)])  
  y_test = test_data[,ncol(data)] 
  
  
  #### Convert the train and test data into xgboost matrix type
  xgboost_train = xgb.DMatrix(data=X_train, label=y_train)
  xgboost_test = xgb.DMatrix(data=X_test, label=y_test)
  
  
  #### Train a model using our training data
  model <- xgboost(data = xgboost_train,                      
                   max.depth = 3, # max depth 
                   nrounds = 50, # max number of boosting iterations
                   verbose = 0) # omit printing 'train-rmse' for each iteration
  
  
  #### Make predictions on the test data
  pred_test <- predict(model, xgboost_test)
  
  
  #### Convert prediction to factor type:
  pred_test[(pred_test>3)] = 3
  pred_y <- as.factor((levels(y_test))[round(pred_test)])
  
  
  #### Create a confusion matrix:
  conf_mat <- confusionMatrix(y_test, pred_y)
  return(conf_mat$overall["Accuracy"])
  
}
```

First we will do XgBoost *without balancing* the data:
```{r}
XgBoost(df_rdr3)
```

We get an Accuracy of around 66%.

```{r}
table(df_rdr3$TRTGRP_values)
```


Now we balance our data with *Downsampling* method, which takes the lowest value (in our case Non-smoker Controls = 193) and makes all classes have the same amount of values:
```{r}
down_data <- downSample(x = df_rdr3[, -ncol(df_rdr3)],
                         y = df_rdr3$TRTGRP_values)
table(down_data$Class) 
```

```{r}
XgBoost(down_data)
```

We get worse results with an Accuracy of 38%.

Now we balance our data with *SMOTE* method, which is an oversampling technique where the  samples are generated for the minority class. 

```{r}
smote_data <- SMOTE(TRTGRP_values ~ ., data  = df_rdr3)                         
table(smote_data$TRTGRP_values) 
```

```{r}
XgBoost(smote_data)
```

We get better results with an Accuracy of 70%.



<br>



### 7.5 GOLDCD + TRTGRP prediction


<span style="color:red">TO DO: finish</span>

Now we will create a new variable *goldcd_trtgrp* which is a combination of the variables GOLDCD + TRTGRP, it will have levels: COPD_2, COPD_3, COPD_4, COPD_9, Non-smoke_Control, Smoke_Control).


Summary of *GOLDCD*:
```{r, eval=FALSE}
table(clean_data$GOLDCD)
```

Summary of *TRTGRP.t1*:
```{r, eval=FALSE}
table(clean_data$TRTGRP.t1)
```




